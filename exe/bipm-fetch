#!/usr/bin/env ruby

require_relative '../lib/bipm-data-importer'

# NOTE: This script uses VCR to cache web requests for fast, deterministic runs.
# To refresh the data from the live website, delete the `cassettes/` directory
# before running this script.

bodies = {
  "JCRB":      'https://www.bipm.org/en/committees/jc/jcrb',
  "JCGM":      'https://www.bipm.org/en/committees/jc/jcgm',
  "CCU":       'https://www.bipm.org/en/committees/cc/ccu',
  "CCTF":      'https://www.bipm.org/en/committees/cc/cctf',
  "CCT":       'https://www.bipm.org/en/committees/cc/cct',
  "CCRI":      'https://www.bipm.org/en/committees/cc/ccri',
  "CCPR":      'https://www.bipm.org/en/committees/cc/ccpr',
  "CCQM":      'https://www.bipm.org/en/committees/cc/ccqm',
  "CCM":       'https://www.bipm.org/en/committees/cc/ccm',
  "CCL":       'https://www.bipm.org/en/committees/cc/ccl',
  "CCEM":      'https://www.bipm.org/en/committees/cc/ccem',
  "CCAUV":     'https://www.bipm.org/en/committees/cc/ccauv',
  "CIPM":      'https://www.bipm.org/en/committees/ci/cipm',
  "CGPM":      'https://www.bipm.org/en/committees/cg/cgpm',
}

BASE_DIR = "data"
a = Mechanize.new

bodies.each do |bodyid, bodyurl|
  next if ARGV[0] == '--fork' && fork
  next if ARGV[0] && ARGV[0].start_with?("--body=") && ARGV[0].downcase != "--body=#{bodyid}".downcase

  body = bodyid.to_s.downcase.gsub(" ", "-").to_sym

  %w[en fr].each do |meeting_lang|
    processed_resolution_urls = Set.new
    next if ARGV[0] == '--fork' && fork

    meeting_lang_sfx     = (meeting_lang == 'fr') ? "-fr" : ""
    meeting_lang_sfx_dir = (meeting_lang == 'fr') ? "-fr" : "-en"
    bodyurl_local = meeting_lang == "en" ? bodyurl : bodyurl.gsub("/en/", "/fr/")

    pages = {}
    VCR.use_cassette("#{body}/#{body}-#{meeting_lang}-pages", record: :new_episodes) do
      pages[:index] = a.get "#{bodyurl_local}"
      pages[:meetings] = a.get "#{bodyurl_local}/meetings"
      pages[:publications] = a.get "#{bodyurl_local}/publications"
      begin
        pages[:recommendations] = a.get "#{bodyurl_local}/recommendations"
      rescue Mechanize::ResponseCodeError; pages[:recommendations] = nil; end
      outcomes_path = bodyid == :CIPM ? "outcomes" : "meeting-outcomes"
      begin
        pages[:outcomes] = a.get "#{bodyurl_local}/#{outcomes_path}"
      rescue Mechanize::ResponseCodeError; pages[:outcomes] = nil; end
    end

    all_meeting_data = []

    pages[:meetings].css('.meetings-list__item').each do |meeting_div|
      date = Bipm::Data::Importer::Common.extract_date(meeting_div.at_css('.meetings-list__informations-date').text)
      title = meeting_div.at_css('.meetings-list__informations-title').text.strip
      href = meeting_div.at_css('.meetings-list__informations-title').attr('href')
      href = "/#{meeting_lang}" + href unless href.start_with? "/#{meeting_lang}/"

      ident = href.split("/#{body}/").last.gsub('/', '.')
      yr = href.include?("/wg/") ? nil : href.split('-').last
      meeting_id = if href.include?("/wg/")
                     wg = href.split("/wg/").last.split("/").first
                     href.split("/").last
                   else
                     parts = href.split("/").last.split("-")
                     parts.length == 2 ? parts[0] : parts[0] + parts[1].sub("_", "-")
                   end

      meeting = VCR.use_cassette("#{body}/#{body}-meeting-#{ident}#{meeting_lang_sfx}", record: :new_episodes) { a.get(href) }
      yr = nil if [bodyid, meeting_id] == [:CIPM, '104-2']

      if yr
        pdf = Bipm::Data::Importer::Common.extract_pdf(meeting, meeting_lang)
        h = { "metadata" => { "title" => title, "identifier" => meeting_id, "date" => date.to_s, "source" => "BIPM - Pavillon de Breteuil", "url" => meeting.uri.to_s }, "pdf" => pdf, "resolutions" => [] }
        h.delete("pdf") unless pdf

        resolution_hrefs = meeting.css(".bipm-resolutions .publications__content").map { |d| d.at_css('a').attr('href') }
        resolutions_additional = pages[:recommendations]&.css(".bipm-resolutions .publications__content")&.select { |d| d.at_css('a').attr('href').include?("/#{ident}/") }&.map { |d| d.at_css('a').attr('href').gsub('/106-2017/', '/104-_1-2015/') } || []

        all_hrefs = (resolution_hrefs + resolutions_additional).uniq.map do |res_href|
          res_href.gsub!('/web/guest/', "/#{meeting_lang}/")
          res_href.sub!("www.bipm.org/", "www.bipm.org/#{meeting_lang}/") unless res_href.include?("/#{meeting_lang}/")
          res_href.gsub!('/104-2015/', '/104-_1-2015/')
          res_href
        end

        all_hrefs.each do |res_href|
          next if processed_resolution_urls.include?(res_href)
          processed_resolution_urls.add(res_href)

          # FINAL FIX: Add error handling for 404s on individual resolution pages.
          begin
            res_id = res_href.split("-").last.to_i
            res = VCR.use_cassette("#{body}/#{body}-recommendation-#{yr}-#{res_id}#{meeting_lang_sfx}", record: :new_episodes) { a.get(res_href) }
            h["resolutions"] << Bipm::Data::Importer::Common.parse_resolution(res, res_id, date, body, meeting_lang, "recommendation?")
          rescue Mechanize::ResponseCodeError => e
            warn "WARNING: Could not fetch #{res_href} (#{e.message}). This link appears to be broken on the BIPM website. Skipping."
          end
        end
        all_meeting_data << h if h["resolutions"].any?
      end

      if meeting_id
        h = { "metadata" => { "title" => title, "identifier" => meeting_id, "date" => date.to_s, "source" => "BIPM - Pavillon de Breteuil", "url" => meeting.uri.to_s, "workgroup" => wg }.compact, "resolutions" => [], "working_documents" => [] }
        decisions_nodes = meeting.css('.bipm-decisions .decisions').to_a
        if pages[:outcomes]
          decisions_nodes += pages[:outcomes].css('.bipm-decisions .decisions').select { |i| i["data-meeting_key"] == meeting_id || i["data-meeting"] == meeting_id }
        end
        decisions_nodes.uniq! { |d| [d.attr('data-meeting'), d.attr('data-number')] }

        h["resolutions"] = decisions_nodes.map do |node|
          node_title = node.at_css('.title-third').text.strip
          type = case node_title; when /\A(Decision|Décision)/ then "decision"; when /\A(Resolution|Résolution)/ then "resolution"; when /\AAction/ then "action"; else "decision"; end
          identifier = "#{node.attr('data-meeting')}-#{node.attr('data-number')}"

          decision_key = [type, identifier]
          next if processed_resolution_urls.include?(decision_key)
          processed_resolution_urls.add(decision_key)

          categories = node.attr('data-decisioncategories') ? JSON.parse(node.attr('data-decisioncategories')).map(&:strip).uniq : []
          r = { "dates" => [date.to_s], "subject" => bodyid.to_s, "type" => type, "title" => node_title, "identifier" => identifier, "url" => meeting.uri.to_s, "categories" => categories, "considerations" => [], "actions" => [] }

          content_html = node.attr('data-text').empty? ? node.css('p').map(&:inner_html).join("\n") : node.attr('data-text')
          content_doc = Nokogiri::HTML(content_html)
          Bipm::Data::Importer::Common.replace_links(content_doc, meeting, meeting_lang)
          part = Bipm::Data::Importer::Common.ng_to_string(content_doc).gsub(%r{<a name="haut">.*?</a>}m, '\1')
          parse = Nokogiri::HTML(part).text.strip

          r["actions"] << { "type" => "approves", "message" => Bipm::Data::Importer::Common.format_message(part) } # Default action
          r
        end.compact

        h["working_documents"] = meeting.css('.portlet-boundary_CommitteePublications_ .publications__content').map do |d|
          d.at_css('.publications__date')&.text&.strip
          { "title" => d.at_css('.title-third').text.strip, "pdf" => d.at_css('.title-third')&.attr("href")&.split('?')&.first }.compact
        end
        all_meeting_data << h if h["resolutions"].any? || h["working_documents"].any?
      end
    end

    parts = all_meeting_data.group_by { |i| [i["metadata"]["workgroup"].to_s, i["metadata"]["identifier"].to_s] }
    parts = parts.sort_by { |(wg,i),| [wg, i.to_i, i] }.to_h

    parts.each do |(wg, mid), hs|
      h = hs.reduce({}) do |memo, item|
        memo.merge(item) { |key, old_val, new_val| key == "resolutions" || key == "working_documents" ? old_val + new_val : new_val }
      end

      h["resolutions"].sort_by! { |i| [i["type"], i["identifier"].to_s.scan(/([0-9]+|[^0-9]+)/).map(&:first).map { |j| j =~ /[0-9]/ ? j.to_i : j }] }
      h.delete("working_documents") if h["working_documents"]&.empty?

      dir_path = wg.empty? ? "#{BASE_DIR}/#{body}" : "#{BASE_DIR}/#{body}/workgroups/#{wg}"
      fn = body == :cgpm ? "#{dir_path}/meetings#{meeting_lang_sfx_dir}/meeting-#{"%02d" % mid}.yml" : "#{dir_path}/meetings#{meeting_lang_sfx_dir}/meeting-#{mid}.yml"

      FileUtils.mkdir_p(File.dirname(fn))
      File.write(fn, YAML.dump(h))
    end

    categories = pages[:publications].css(".publications").map do |i|
      title = i.previous_element&.text&.strip || "Misc"
      { "title" => title, "documents" => i.css(".publications__content").map { |d| { "title" => d.at_css(".title-third").text.strip.gsub(/\s+/, ' '), "pdf" => d.at_css(".title-third")&.attr("href")&.split('?')&.first }.compact } }
    end
    unless categories.empty?
      doc = { "metadata" => { "url" => bodyurl.gsub("/en/", "/#{meeting_lang}/") + "/publications/" }, "categories" => categories }
      fn = "#{BASE_DIR}/#{body}/publications-#{meeting_lang}.yml"
      File.write(fn, YAML.dump(doc))
    end

    puts "* #{bodyid}/#{meeting_lang} parsing done"
    exit! if ARGV[0] == '--fork'
  end

  2.times { Process.wait } if ARGV[0] == '--fork'
  exit! if ARGV[0] == '--fork'
end

bodies.length.times { Process.wait } if ARGV[0] == '--fork'